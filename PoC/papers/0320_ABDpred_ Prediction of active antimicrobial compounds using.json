{
  "pmid": "38345040",
  "title": "ABDpred: Prediction of active antimicrobial compounds using supervised machine learning techniques.",
  "abstract": "BACKGROUND OBJECTIVES: Discovery of new antibiotics is the need of the hour to treat infectious diseases. An ever-increasing repertoire of multidrug-resistant pathogens poses an imminent threat to human lives across the globe. However, the low success rate of the existing approaches and technologies for antibiotic discovery remains a major bottleneck. In silico methods like machine learning (ML) deem more promising to meet the above challenges compared with the conventional experimental approaches. The goal of this study was to create ML models that may be used to successfully predict new antimicrobial compounds. METHODS: In this article, we employed eight different ML algorithms namely, extreme gradient boosting, random forest, gradient boosting classifier, deep neural network, support vector machine, multilayer perceptron, decision tree, and logistic regression. These models were trained using a dataset comprising 312 antibiotic drugs and a negative set of 936 non-antibiotic drugs in a five-fold cross validation approach. RESULTS: The top four ML classifiers (extreme gradient boosting, random forest, gradient boosting classifier and deep neural network) were able to achieve an accuracy of 80 per cent and above during the evaluation of testing and blind datasets. INTERPRETATION CONCLUSIONS: We aggregated the top performing four models through a soft-voting technique to develop an ensemble-based ML method and incorporated it into a freely accessible online prediction server named ABDpred ( http://clinicalmedicinessd.com.in/abdpred/ ).",
  "journal": "The Indian journal of medical research",
  "year": "2024",
  "authors": [
    "Jana T",
    "Sarkar D",
    "Ganguli D",
    "Mukherjee S",
    "Mandal R"
  ],
  "doi": "10.4103/ijmr.ijmr_1832_22",
  "mesh_terms": [
    "Humans",
    "Algorithms",
    "Machine Learning",
    "Anti-Infective Agents",
    "Supervised Machine Learning",
    "Anti-Bacterial Agents"
  ],
  "full_text": "## Material & Methods\nThis study was undertaken at the division of Clinical Medicine, ICMR-National Institute of Cholera and Enteric Diseases, Kolkata, West Bengal, India between June 2021 to August 2022. It employed a prediction modelling approach by using data archived on different databases. In this study, ML classifiers such as gradient boosting, random forest, deep learning, and support vector machine were employed in different research groups101112. The learning parameters tuning, cross-validation approach, regularization, and different positive and negative ratio mixed training were done here exclusively. Furthermore, the ABDpred prediction server was used to provide the output of the used machine learning models in an HTML framework.\nDataset: For the training, testing and blind datasets, a total of 390 antibiotic drugs were curated from the DrugBank database (https://go.drugbank.com/) and 1170 FDA-approved drugs were randomly chosen from the FDA drug database (https://www.fda.gov/drugs, accessed on June 23, 2021). The positive dataset (class 1) comprised of antibiotic drugs, while the negative dataset (class 0) was composed of randomly collected, FDA-approved drugs that had no antibiotic, antifungal or antibacterial activity reported in the ChEMBL database14. No two drugs were exactly the same in the training and testing datasets. Multidimensional scaling (MDS) in two-dimensional (2D) space was done separately for the positive and negative classes with the chosen Tanimoto similarity cut-off value of 0.4 using ChemMine tool15. All datasets (positive and negative) were divided into 80 per cent for training and 20 per cent for blind validation. The models were developed using a positive set of 312 antibiotic drugs and a negative set of 936 non-antibiotic drugs. We randomly selected data from the total dataset by varying the ratio of positives to negatives to 1:1, 1:2 and 1:3 to avoid bias towards positive or negative. In the same manner, the blind set was created with the same three ratios of positive to negative sets (78 positive drugs and 234 negative drugs). The blind set of drugs was never used in training or testing. The blind dataset played a crucial role in identifying the best mathematical model among all the developed models. An independent dataset consisting of 173,714 small chemicals from the NCI was curated for drug screening purposes and to evaluate the performance of the best models16.\nMolecular descriptors as features: Molecular descriptors, including physicochemical and structural properties, have been widely used in the drug discovery research to characterize drugs and small chemicals17. The features of antibiotic and non-antibiotic drugs were computed using python modules, such as PaDELPy and PubChemPy1819. The two modules returned 1440 and 17 features, respectively, as the vector length. The 1440 descriptors available in the PaDELPy module included acidic group count, ALogP, APol, aromatic atom count, aromatic bond count, atom count, autocorrelation, Barysz matrix and basic group count. The PubChemPy provided 17 important descriptors to help characterize molecules, including molecular weight, xlogp3-aa, hydrogen bond donor count, hydrogen bond acceptor count, rotatable bond count, exact mass, monoisotopic mass, topological polar surface area, heavy atom count, formal charge, complexity, isotope atom count, defined atom stereocenter count, undefined atom stereocenter count, defined bond stereocenter count, undefined bond stereocenter count and covalently bonded unit count. By employing these descriptors, one can achieve a thorough understanding of the molecular structure and characteristics of a particular compound. Three well-known feature selection methods, namely univariate, L1 based and tree based, were used to train and test the models feature wise (17 and 1440) as well as at different ratios (1:1, 1:2 and 1:3) of the training and testing sets. The models were also tested on the blind dataset simultaneously to get the best-optimized models with selective features. For the above two vector lengths (17 and 1440), the method was continued without feature selection being done separately (all features). Besides, the importance of features study was also performed based on the ranks, using a supervised ML technique called ExtraTreesClassifier20.\nFive-fold cross-validation (5-fold CV): In five iterations, 80 per cent of the samples were used for training and the remaining 20 per cent were used for testing, making five-fold cross-validation (5-fold CV) a popular technique for selecting the best-trained models among all models. The average score was considered the final performance score of the trained model. The actual purpose of five-fold cross-validation is to identify the best learning parameters of all ML algorithms, which is helpful to select the best model. The blind dataset was also screened using the best-performing models based on sensitivity, specificity, accuracy, precision, F1 score, Matthews correlation coefficient (MCC) and area under the ROC curve (AUC) value. These matrices are computed by 2x2 confusion matrix, that contains true positive (TP), true negative (TN), false negative (FN), and false positive (FP).\nSensitivity = TP/(TP+FN);\nSpecificity= TN/(TN+FP);\nPrecision = TP/(TP+FP);\nAccuracy = (TP+TN)/(TP+TN+FP+FN);\nF1 score = 2TP/(2TP+FP+FN);\nMCC = (TP*TN-FP*FN)/ \u221a(TP+FP) (TP+FN) (TN+FP) (TN+FN)\nMachine learning classifiers: The identification of antibiotic drugs can be viewed as a binary classification system, with class 1 representing drugs with antibiotic activity and class 0 representing drugs with non-antibiotic activity. Different mathematical models were developed in 5-fold CV using the supervised classifiers, such as extreme gradient boosting, random forest (RF), gradient boosting classifier (GBC), deep neural network, support vector machine (SVM), multilayer perceptron, decision tree (DT) and logistic regression (LR), to screen antibiotics from the non-antibiotic drugs available in the public domain (ChEMBL)14. Each of the classifiers tuned its specific parameters iteratively to get the optimized scores. The chances of over-fitting were also minimized using 5-fold CV and regularization parameters (L1 and L2 regularization).\nExtreme gradient boosting (XGBoost): In this study, supervised ensemble XGBoost ML was used as a classifier model. It is a well-performing ML that invokes a multi-layer DT-based system, which is potentially easily interpretable. During 5-fold CV, XGBoost was trained with the following parameter settings: maximum depth equal to 5 (max_depth), learning rate equal to 0.1 (learning_rate), number of tree estimators set to 1000 (n_estimators), the value of L2 and L1 regularization parameter lambda set to 1 (reg_lambda) and 0 (reg_alpha) with a subsample was set to 0.8 to prevent overfitting. However, these parameters and threshold values were tuned with grid search to get optimized predicted model performance.\nRandom forest (RF): RF is another supervised ensemble ML classifier that classifies by constructing multiple DT. During training, several uncorrelated trees create a committee-like environment that has a higher ability for prediction than individual trees. In five-fold cross-validation, different parameters of RF were set, such as estimator (n_estimators) was set to 200, random state (random_state) was set to 92, bootstrap was enabled and max_features was set to sqrt to get better performance of the model. Different threshold values were tuned simultaneously. These parameters of RF were also computed in the grid searching technique to get optimum parameter values with model performance.\nGradient boosting classifier (GBC): GBC is an ensemble ML that can also classify by building many DT of fixed size, where the boosting has the ability to provide a high quality of learning of the DTs. The parameters set during 5-fold CV of GBC included random state (random_state) set at 42, loss of function set to deviance, learning rate equal to 0.1 and n_estimator set to 100, which were computed to get better performance of the model.\nDeep neural network (DNN): DNN classifier is a subgroup of ANN and artificial intelligence. DNN functions by creating several layers of neural networks, including input, output and hidden layers. It is a robust and powerful tool, nowadays applied extensively to solve complex classification problems. The \u2018TensorFlow (version: 2.4.0)\u2019 and \u2018Keras\u2019 deep-learning python packages were used to predict antibiotic and non-antibiotic drugs. The DNN was built in a sequential network model with dense layers. In this model, several parameters such as kernel initializer (set to uniform), activation functions (relu and sigmoid), optimizer (adam), loss of function (binary_crossentropy) and dropout (0.45) were used to train and test with five-fold cross-validation.\nSupport vector machine (SVM): SVM is a supervised binary ML classifier capable to handle non-linear and complex data using kernel functions (RBF, sigmoid and polynomial). Different parameters, such as cost (C) equal to 350, gamma = 0.00001 and kernel set to radial basis function (RBF), were set to get well-performing model.\nMulti-layer perceptron (MLP): MLP is a three-layered feedforward traditional neural network with input, hidden and output layers. In the input layer of the nodes, there exists an activation function. MLP performs using backpropagation in training. The parameters such as hidden layer sizes and maximum iteration set to 100, random state set at 62 and batch size equal to 320 were used to develop the ML model.\nDecision tree (DT): DT is another popular supervised ML classifier that can perform classification using the tree model of decision-making, computed based on cost, outcome and utility. DT consists mainly of three nodes such as decision, chance and end nodes.\nLogistic regression (LR): LR is a probability-based ML classifier that performs binary classification and is assigned a probability score between 0 and 1.\nPython packages: The features of positive antibiotic drugs and negative drugs were curated using \u2018PubChemPy\u2019 and \u2018PaDELPy\u2019. The feature selection, model development, model selection, computing model performance, best model selection and best model evaluation were done using the \u2018scikit-learn\u2019, \u2018tensorflow\u2019, \u2018keras\u2019 and \u2018XGBoost\u2019 packages in Python 3.8.6.\nWebserver: ABDpred web server was developed to predict antibiotic drugs using four best-performing ML classifiers, XGBoost RF, GBC and DNN. The web pages of the server were designed using PHP (7.3.25), HTML and JavaScript. In the back end of the server, Python 3.8.6 was embedded inside CentOS Linux 7 (core). The aggregation of four ML classifiers was done using a soft-voting technique for better prediction results. The consensus prediction scores (x\u2019 and y\u2019) were computed by taking the mean prediction scores of the four ML classifiers in terms of positiveness and negativeness. Source codes of ABDpred are available at http://clinicalmedicinessd.com.in/abdpred/download.php\nWhere, n is equal to 4 (top 4 performing ML classifiers) and Z equals to input vectors (features).\n\n## Results\nFeature importance and molecule descriptor selection: To identify the important features for developing the ML classifier models, different combinations of features (PubChemPy and PaDELPy) and different ratios of positive and negative data (1:1, 1:2 and 1:3) were applied to the training and blind datasets to generate a high value of area under the curve (AUC). Three feature selection algorithms of the python \u2018scikit-learn\u2019 package, namely the univariate, L1-based and tree-based algorithms, were used for this purpose. With Chi-square-based univariate feature selection (percentile) and PubChemPy python module, the XGBoost achieved an AUC value of 81.52 per cent on the training set and 45.62 per cent on the blind dataset with positive:negative (P:N) ratio of 1:1, and 5 features were identified as vector length (Table I). In contrast, when the univariate feature selection (percentile) method was applied to the python module PaDELPy, the XGBoost achieved an AUC value of 86.63 and 61.97 per cent on the training and blind dataset, respectively. We identified 351 features as vector length when positive and negative datasets were at an equal ratio (1:1). When all features were considered (no feature selection was done) in the PubChemPy module, XGBoost achieved an AUC value of 88.39 per cent on the training set and 80 per cent on the blind set with P:N ratio of 1:1, and 17 features were identified. A similar trend was observed with all features in the PaDELPy module. It was observed that the AUC scores decreased when the ratio of the positives and negatives was increased. Interestingly, AUC scores of the XGBoost model did not increase further, if more than 17 features were considered. Similar observations were made with RF and GBC, as shown in Supplementary Tables SI and SII. By using Extra Tree Classifier, a powerful ensemble-based ML method, we increased our assurance in the 17 significant features identified, based on their rankings, as shown in Figure 2. The 17 features of the python module PubChemPy that achieved a more balanced AUC value on the training, as well as the blind dataset, were chosen for model development and analysis.\nSignificant differences between the antibiotic and non-antibiotic drugs for the above features were observed, as shown in Table II. The median and mean value differences underscored that normal data distribution was not possible. This observation was important since it was extensively reported in the recent past21. On a MDS at 2D plane, using ChemMine tool with Tanimoto similarity cut-off of 0.4 for the positive as well as the negative set, we found that most of the positive drugs were clustered below the threshold value of 0.3 for the similarity score, while the negative drugs were largely clustered below the cut-off value of 0.33 (Supplementary Fig. S1). The distribution plots of all the 17 features for both the positive and negative sets were done (Supplementary Fig. S2).\nPerformance comparison of different classifiers: We used different classifiers, such as XGBoost, RF, GBC, DNN, SVM, MLP, DT and LR, to develop models and computed their performance measures based on diverse parameters (parameters tuning). The best results of all ML classifiers are reported here. Different threshold values of these ML classifiers were also calculated to achieve the most promising and optimized results. All the ML classifiers performed best at the threshold value of 0.5 (Supplementary Table SIII). The performance measures were also compared to each of the other ML classifiers. To evaluate model performances, stratified k-fold computations (k=3, 5 and 10) were conducted simultaneously (Supplementary Table SIV). The XGBoost and RF performed better than GBC, DNN, SVM, MLP, DT and LR (Table III). The XGBoost achieved the sensitivity and specificity values of 87.42 and 89.35 per cent, respectively, with an accuracy value of 88.39 per cent, MCC value of 76.51 per cent, precision value of 89.57 per cent, F1 score of 88.16 per cent and AUC value of 88.38 per cent at the threshold value of 0.5. The RF achieved the sensitivity, specificity, accuracy, MCC and precision values of 87.74, 89.35, 88.55, 77.48 and 89.53 per cent, respectively, with an F1 score of 88.41 per cent and AUC value of 88.55 per cent at the threshold 0.5 compared to other ML classifiers, the performance of XGBoost and RF was more balanced (Fig. 3). The MCC values showed that the RF model outperformed XGBoost when performances of the two models were compared. DTs of XGBoost and RF were generated to interpret the decision rules, using the selected 17 features on 312 antibiotic (positive) and 312 non-antibiotic (negative) drugs (Supplementary Fig. S3 and S4).\nModel performance using the blind set: The blind set was tested using the best models of all ML classifiers used in this study. As shown in Table IV, the RF model had a sensitivity of 68.75 per cent, specificity of 92.5 per cent, accuracy of 80.62 per cent, MCC of 60.12 per cent, precision of 90.16 per cent and AUC of 80 per cent, outperforming all other ML classifiers. The XGBoost model achieved the sensitivity (70%), specificity (90%), accuracy (80%), MCC (59.39%), precision (87.5%) and AUC (80%) values, which were quite similar to RF. These two models performed better than GBC, DNN, SVM, MLP, DT and LR classifiers. However, DNN and GBC also did reasonably well with sensitivity over 68 per cent, accuracy above 78 per cent and AUC value higher than 78 per cent. Interestingly, we found that DNN performed better when the number of features increased. Finally, considering the results of both the training set and the blind set, XGBoost, RF, GBC and DNN were chosen for further analysis and webserver development.\nModel performance using the training set as negative/unknown: XGBoost, RF, GBC and DNN models were also used to compute the performance of the training set (312 antibiotic drugs). The reason for doing this analysis was that the models were evaluated again on a negative or unknown level set to ensure that these could learn the underlying patterns in the data and make accurate predictions on unseen data. This helps to prevent overfitting, which can lead to poor performance on unseen data. The top four models identified positive drugs as positive with higher accuracy than the other models. The models performed well with a threshold (cut-off) probability set at 0.56 for the training set taken as a negative dataset (Supplementary Fig. S5).\nModel performance on 14 widely used antibiotic drugs: The 14 well known antibiotic drugs, such as penicillin G, ampicillin, cloxacillin, vancomycin, ticarcillin, meropenem, azithromycin, ceftazidime, daptomycin, rifampicin, chloramphenicol, ciprofloxacin, tetracycline and gentamicin c1a, were evaluated using the top four ML classifiers: XGBoost, RF, GBC and DNN. As shown in Figure 4, the average cut-off probability was 0.56, but XGBoost predicted these antibiotic drugs with a probability score above 0.9.\nModel performance on independent dataset: XGBoost, RF, GBC and DNN models were tested for their performance on the NCI dataset of small chemicals (n=173714), taken as an independent set. Both XGBoost and RF performed better than GBC and DNN, with an accuracy of above 80 per cent, while XGBoost was even better as compared to RF. The best XGBoost model predicted a total of 13,552 small chemicals as antibiotic drugs with a probability score better than 0.9, out of which 596 chemicals reached the same score for both RF and XGBoost (Supplementary Fig. S6). To validate the prediction result, 596 NCI small chemicals were studied individually in literature. Interestingly, it was found that the majority of the above 596 NCI chemicals have antibacterial activity in literature (Supplementary Table SIV).\nWebserver prediction: The ABDpred webserver is capable of computing structural similarities of the input chemicals to 390 known antibiotic drugs based on the Tanimoto Coefficient score. At the home page of the webserver, users can provide the PubChem CID of choice in the text box (Fig. 5A). Once the prediction button is pressed, the webserver would process the inputs and generate a result page in return, named as \u2018prediction_result_updated.php\u2019. The result page will contain the output arranged into four sections. In the first section, called \u2018Input compound brief details\u2019, the user will get input compound CID, structure and synonymous name, as shown in Figure 5B (for example, gramicidin D, CID: 42567103). The next section illustrates \u2018drug-like properties and bioavailability profile of the input compound\u2019 using Lipinski\u2019s rule, Pfizer\u2019s 3/75 rule, Veber\u2019s rule and Egan\u2019s rule2223. Another section describes \u2018prediction results given by different ML classifiers (such as XGBoost, RF, GBC and DNN)\u2019 (Fig. 5C). A fourth section mentions \u2018Aggregation of prediction results\u2019 (Fig. 5D). Finally, a separate section demonstrates the search results of similarity of the input compound to known antibiotic drugs used in this study (Fig. 5E). ABDpred is freely accessible at http://clinicalmedicinessd.com.in/abdpred/.\nComparison of ABDpred server with the existing machine learning (ML) methods: Ivanenkov\u2019s12 group developed ML models to predict antibacterial chemicals. They also validated some of the chemicals having the highest predicted antibacterial potency by in vitro and in vivo experiments12. Stokes\u2019s13 group developed Chemprop webserver that was used to discover halicin as an antimicrobial compound. The Chemprop webserver only achieved 51.5 per cent of accuracy and 51.5 per cent of sensitivity, while using the ABDpred server, we screened these chemicals with the top performing models and found the best model (RF) to have 67 per cent of accuracy and 67 per cent of sensitivity in identifying compounds with antimicrobial activity (Supplementary Table SVI). In addition, most of the compounds were identified as likely antibiotic drugs, and their drug likeliness was determined based on 17 selected features.\n\n## Discussion\nMDR and extensively drug-resistant bacterial pathogens have emerged as a major threat to public health1. There is a growing global demand for newer antimicrobial drug discovery since the projected cost of antimicrobial resistance in terms of loss of human lives, economic impact and burden on the health system is enormous. However, the traditional drug discovery pipelines are rather expensive and slow, normally costing billions of dollars, taking years to enter the clinical trial phase and are unlikely to catch up with the fast-evolving pathogens, acquiring resistance through multiple mechanisms. Under such circumstances, computational approaches, especially ML techniques, can yield double benefits of speeding up the drug discovery process considerably while lowering the overall cost involved in compound screening and bioactivity prediction1224.\nSeveral models have been developed to predict the antibacterial activities of small-molecule compounds. However, some models are focused on individual classes of compounds825, and hence, are not applicable to diverse compound libraries. Others, being restricted to individual organisms1326, are not helpful for the discovery of broad-spectrum antimicrobials. Although models for the prediction of antibacterial drug from heterogeneous molecules exist611, significant scope remains for their improvement through the use of advanced ML algorithms such as XGBoost, RF and DNN.\nFor every ML algorithm, it is critical to prudently choose positive and negative datasets. It is difficult to build a reliable model without considerable differences between the features of the positive and the negative data. We achieved this by curating 390 antibiotic drugs from the DrugBank database as the positive set, while 1170 randomly chosen FDA-approved drugs that lack any kind of antimicrobial functions (antiviral, antifungal, antiprotozoal and antiparasitic) confirmed by ChEMBL database14 were considered the negative set. More than 80 per cent of drugs from the positive dataset were clustered at the Tanimoto coefficient value <0.3 in the MDS using ChemMine, indicating that these had diverse structures. In contrast, the negative dataset had a Tanimoto coefficient value below 0.33 in MDS for 80 per cent of the compounds. Previous studies by Yang\u2019s11 group used 230 antibacterial and 381 non-antibacterial compounds. Masalha\u2019s6 group used a set of 628 anti-bacterial drugs from the Comprehensive Medicinal Chemistry database as positives and another 2,892 phytochemicals from Analyticon Discovery GmbH as the negative dataset. However, none of the above studies excluded antimicrobial compounds from the negative dataset. On the other hand, Stokes\u2019s13 group screened 2,335 unique compounds, including 1,760 molecules from the FDA-approved drug library and 800 natural products isolated from plant, animal and microbial sources for growth inhibition against E. coli BW25113 and used the positive targets as the training dataset.\nIn Stokes\u2019s13 group study only known drug molecules for the construction of both positive and negative datasets because, first, their drug likeliness and bioavailability are already known, second, many of these have passed clinical trials and, third, majority of them are currently available in the market. Our approach might prove useful in light of considerable interest in repurposing already-marketed drugs as antibiotics, which would cut down cost, time and effort for clinical trials.\nIn this study, 80 per cent of the positive and negative datasets were used for training and testing by stratified 5-fold cross-validation, and two other types of k-fold cross-validation (k=3 and, k=5), while 20 per cent of the data were kept aside to be used as a blind set for model validation purposes. Although Yang\u2019s11 group followed a similar approach, Masalha\u2019s6 group used only 3-fold cross-validation, which is less than optimum to ensure proper shuffling of the data to get rid of the inherent bias in the dataset. In both training and blind datasets, three different positive-to-negative data ratios, such as 1:1, 1:2 and 1:3, were used to check the overfitting of the models. We observed that the balanced dataset with an equal number of positive and negative instances performed better than the unbalanced sets. Other studies did not check the effects of balanced and unbalanced datasets on the performance of the prediction algorithms.\nIn this study 2D physicochemical properties comprising 17 features (PubChemPy) were compiled and all (1440) features (PaDELpy) of the datasets. In contrast, Yang\u2019s11 group used only 198 manually selected molecular descriptors, while Stokes\u2019s13 group combined graph representation of each molecule with molecular features computed by RDKit. A combination of univariate, L1-based and tree-based feature selection methods were used in this study, whereas previous studies had used only a single method like recursive feature elimination11 or iterative stochastic elimination6.\nOur study employed eight ML classifiers, namely XGBoost, RF, GBC, DNN, SVM, MLP, DT and LR, to construct the prediction models, from which the top four models, namely XGBoost, RF, GBC and DNN, were chosen for further analysis. Yang\u2019s11 group used three algorithms, namely SVC, k-NN and C4.5 DT, for their study, whereas the Chemprop server developed by Stokes\u2019s13 group was solely based on DNN.\nOur antibiotic prediction server, ABDpred, used a soft-voting technique for the first time for antibiotic discovery, using the aggregation of top four classifiers (XGBoost, RF, GBC and DNN). Soft voting is capable of yielding improved prediction results because it determines the classifier that has highest influence on the final ensemble decision by considering each classifier\u2019s uncertainty in the final decision process27. ABDpred server is also capable to check the information on the drugability and bioavailability of the input chemicals. At the same time, the server also compares the structural similarity of the input drugs/chemicals to known antibiotics based on the Tanimoto similarity scores (cut-off 0.6). A higher similarity score hints towards a greater potential of the input drug/chemical to be used as an antimicrobial compound in the future. The Chemprop server, the only other existing web server with a similar functionality, does not provide this sort of information about the input chemicals.\nWe have also evaluated the NCI chemical database (n=173714) using the XGBoost and RF models and predicted 596 NCI chemicals as potential antibiotics. Interestingly, we found that most of these predicted molecules had literature evidence for antibiotic activity2829. Further, we tested our top four models to see if they could accurately predict the antibiotic activity of more than 10 compounds reported by Ivanenkov\u2019s12 group, including halicin, which Stokes\u2019s13 group had predicted as a potent antibiotic. Our best RF model classified with 67 per cent of accuracy, 67 per cent of sensitivity and 67 per cent of AUC and these compounds as antimicrobial compounds.\nOverall, developing ML classifiers that can accurately identify antibacterial compounds from chemical databases would be advantageous for drug development. Through this study a ML-based method was developed with the aggregation of four classifiers. The RF model was the best performer among all four classifiers, achieving an accuracy value of 88.55 per cent on the training test and 80.62 per cent on the blind dataset for predicting antimicrobial compounds. Our models predicted 596 small chemicals from the NCI chemical database as future antimicrobial compounds with high confidence and a probability score of higher than 0.9. Most of these predicted chemicals had already reported antibacterial activity and might be developed as antimicrobial compounds in the near future. Overall, this will be a helpful resource for people working in the field of antibiotic research and development, including pharmaceutical companies, clinical researchers and healthcare providers.\n\n## Financial support and sponsorship\nThis study received funding supported from the Indian Council of Medical Research (No. 2019-3127/GEN/ADHOC-BMS) to SD.\n\n## Conflicts of interest\nNone.",
  "has_full_text": true
}